{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e38a075",
   "metadata": {},
   "source": [
    "# Module 10: Two-Tower Models\n",
    "\n",
    "This is a basic two-tower recommender model that was trained on the 1 million MovieLens dataset.  This code was initially generated by ChatGPT (OpenAI 2025), before I debugged it and tested its expected behavior.\n",
    "\n",
    "OpenAI. 2025. Chat with ChatGPT about Two-Tower Recommendation Systems and Evaluation Metrics. ChatGPT, June 3, 2025. https://chat.openai.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "630404b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 503.39047760243915\n",
      "Epoch 2, Loss: 393.6391291313464\n",
      "Epoch 3, Loss: 309.2055415863271\n",
      "Epoch 4, Loss: 242.90896415222636\n",
      "Epoch 5, Loss: 190.5340214995167\n",
      "Epoch 6, Loss: 148.98679762545143\n",
      "Epoch 7, Loss: 116.00413230252083\n",
      "Epoch 8, Loss: 89.93755259599222\n",
      "Epoch 9, Loss: 69.41950952183561\n",
      "Epoch 10, Loss: 53.34633196955142\n",
      "Epoch 11, Loss: 40.82120997338649\n",
      "Epoch 12, Loss: 31.11166368664988\n",
      "Epoch 13, Loss: 23.640140599302015\n",
      "Epoch 14, Loss: 17.930266747389304\n",
      "Epoch 15, Loss: 13.60136778946118\n",
      "Epoch 16, Loss: 10.345775104239774\n",
      "Epoch 17, Loss: 7.902506875869868\n",
      "Epoch 18, Loss: 6.072773063274296\n",
      "Epoch 19, Loss: 4.700501372442221\n",
      "Epoch 20, Loss: 3.679887526175555\n",
      "Epoch 21, Loss: 2.9311759567931484\n",
      "Epoch 22, Loss: 2.379806975422003\n",
      "Epoch 23, Loss: 1.9715879445185747\n",
      "Epoch 24, Loss: 1.6699375293748764\n",
      "Epoch 25, Loss: 1.450127925561822\n",
      "Epoch 26, Loss: 1.292082395623712\n",
      "Epoch 27, Loss: 1.1817159456060367\n",
      "Epoch 28, Loss: 1.0970031521509371\n",
      "Epoch 29, Loss: 1.0320135825277899\n",
      "Epoch 30, Loss: 0.9874032058984118\n",
      "Epoch 31, Loss: 0.9547957530259477\n",
      "Epoch 32, Loss: 0.9337414394101828\n",
      "Epoch 33, Loss: 0.9150671994747104\n",
      "Epoch 34, Loss: 0.8992237530248549\n",
      "Epoch 35, Loss: 0.886580132310043\n",
      "Epoch 36, Loss: 0.8778540597242468\n",
      "Epoch 37, Loss: 0.87054284682969\n",
      "Epoch 38, Loss: 0.863430718479254\n",
      "Epoch 39, Loss: 0.8632283168832969\n",
      "Epoch 40, Loss: 0.8589752028360391\n",
      "Epoch 41, Loss: 0.8542108355885576\n",
      "Epoch 42, Loss: 0.849978010139197\n",
      "Epoch 43, Loss: 0.8458547392464659\n",
      "Epoch 44, Loss: 0.8463959836441538\n",
      "Epoch 45, Loss: 0.8431133733838415\n",
      "Epoch 46, Loss: 0.8414396081129303\n",
      "Epoch 47, Loss: 0.8400922676791316\n",
      "Epoch 48, Loss: 0.8406867958090799\n",
      "Epoch 49, Loss: 0.8374362985801209\n",
      "Epoch 50, Loss: 0.8356913630767246\n",
      "Precision@10: 0.0030\n",
      "Recall@10: 0.0008\n",
      "NDCG@10: 0.0029\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load and preprocess data (table-wise sharding)\n",
    "ratings = pd.read_csv('./data/ml-1m/ratings.dat', sep='::', header=None,\n",
    "                      names=['user_id', 'movie_id', 'rating', 'timestamp'], engine='python')\n",
    "movies = pd.read_csv('./data/ml-1m/movies.dat', sep='::', header=None,\n",
    "                     names=['movie_id', 'title', 'genres'], engine='python', encoding=\"latin-1\")\n",
    "users = pd.read_csv('./data/ml-1m/users.dat', sep='::', header=None,\n",
    "                    names=['user_id', 'gender', 'age', 'occupation', 'zip_code'], engine='python')\n",
    "\n",
    "# Table-wise data sharding (easy approach)\n",
    "data = pd.merge(ratings, movies[['movie_id', 'title']], on='movie_id')\n",
    "\n",
    "user_to_idx = {user: idx for idx, user in enumerate(data['user_id'].unique())}\n",
    "movie_to_idx = {movie: idx for idx, movie in enumerate(data['movie_id'].unique())}\n",
    "\n",
    "data['user_idx'] = data['user_id'].map(user_to_idx)\n",
    "data['movie_idx'] = data['movie_id'].map(movie_to_idx)\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dataset\n",
    "class RecDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.user_idx = torch.tensor(data['user_idx'].values)\n",
    "        self.movie_idx = torch.tensor(data['movie_idx'].values)\n",
    "        self.rating = torch.tensor(data['rating'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_idx)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_idx[idx], self.movie_idx[idx], self.rating[idx]\n",
    "\n",
    "train_dataset = RecDataset(train_data)\n",
    "test_dataset = RecDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "# Two-Tower Model\n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, n_users, n_movies, emb_dim=512):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
    "        self.movie_emb = nn.Embedding(n_movies, emb_dim)\n",
    "\n",
    "    def forward(self, user_idx, movie_idx):\n",
    "        user_embedding = self.user_emb(user_idx)\n",
    "        movie_embedding = self.movie_emb(movie_idx)\n",
    "        scores = (user_embedding * movie_embedding).sum(dim=1)  # dot product\n",
    "        return scores\n",
    "\n",
    "model = TwoTowerModel(n_users=len(user_to_idx), n_movies=len(movie_to_idx))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for user_idx, movie_idx, rating in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(user_idx, movie_idx)\n",
    "        loss = loss_fn(output, rating)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    #if epoch + 1 % 5 == 0:\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "# Evaluation: NDCG@10, Precision@10, Recall@10\n",
    "def evaluate_topk(model, test_df, k=10):\n",
    "    model.eval()\n",
    "    user_positive_items = defaultdict(set)\n",
    "    for _, row in test_df.iterrows():\n",
    "        user_positive_items[row['user_idx']].add(row['movie_idx'])\n",
    "\n",
    "    precision_list, recall_list, ndcg_list = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for user in user_positive_items.keys():\n",
    "            # Get all movie scores for this user\n",
    "            user_tensor = torch.tensor([user] * len(movie_to_idx))\n",
    "            movie_tensor = torch.tensor(list(movie_to_idx.values()))\n",
    "            scores = model(user_tensor, movie_tensor).cpu().numpy()\n",
    "\n",
    "            #topk_indices = np.argsort(scores)[-k:][::-1]\n",
    "\n",
    "            topk_indices = np.argsort(scores)[-k:][::-1].copy()\n",
    "            topk_items = movie_tensor[topk_indices].numpy()\n",
    "\n",
    "            #topk_items = movie_tensor[topk_indices].numpy()\n",
    "\n",
    "            true_items = user_positive_items[user]\n",
    "            hits = [1 if item in true_items else 0 for item in topk_items]\n",
    "\n",
    "            precision = np.sum(hits) / k\n",
    "            recall = np.sum(hits) / len(true_items)\n",
    "            dcg = np.sum([rel / np.log2(idx + 2) for idx, rel in enumerate(hits)])\n",
    "            idcg = np.sum([1 / np.log2(i + 2) for i in range(min(len(true_items), k))])\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "            ndcg_list.append(ndcg)\n",
    "\n",
    "    print(f\"Precision@{k}: {np.mean(precision_list):.4f}\")\n",
    "    print(f\"Recall@{k}: {np.mean(recall_list):.4f}\")\n",
    "    print(f\"NDCG@{k}: {np.mean(ndcg_list):.4f}\")\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_topk(model, test_data, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8062917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.0030\n",
      "Recall@10: 0.0008\n",
      "NDCG@10: 0.0029\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "evaluate_topk(model, test_data, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "709d9db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movie_id                                              title\n",
      "0      2873                          Lulu on the Bridge (1998)\n",
      "1       599                             Wild Bunch, The (1969)\n",
      "2      3326                   What Planet Are You From? (2000)\n",
      "3      2415                         Violets Are Blue... (1986)\n",
      "4      3167                            Carnal Knowledge (1971)\n",
      "5      2737                               Assassination (1987)\n",
      "6      3636  Those Who Love Me Can Take the Train (Ceux qui...\n",
      "7      1322            Amityville 1992: It's About Time (1992)\n",
      "8      2349                                   Mona Lisa (1986)\n",
      "9      1507                               Paradise Road (1997)\n"
     ]
    }
   ],
   "source": [
    "def recommend_for_user(model, user_id_raw, user_to_idx, movie_to_idx, movies_df, ratings_df, k=10):\n",
    "    model.eval()\n",
    "    \n",
    "    if user_id_raw not in user_to_idx:\n",
    "        print(f\"User ID {user_id_raw} not found.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    user_idx = user_to_idx[user_id_raw]\n",
    "    all_movie_idxs = np.array(list(movie_to_idx.values()))\n",
    "    \n",
    "    # Get the set of movie_idx the user has already seen\n",
    "    watched_movie_ids = ratings_df[ratings_df['user_id'] == user_id_raw]['movie_id'].unique()\n",
    "    watched_movie_idxs = [movie_to_idx[movie_id] for movie_id in watched_movie_ids if movie_id in movie_to_idx]\n",
    "    \n",
    "    # Filter out watched movies\n",
    "    unwatched_mask = ~np.isin(all_movie_idxs, watched_movie_idxs)\n",
    "    candidate_movie_idxs = all_movie_idxs[unwatched_mask]\n",
    "    \n",
    "    if len(candidate_movie_idxs) == 0:\n",
    "        print(\"No unseen movies to recommend.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Score all candidate movies\n",
    "    user_tensor = torch.tensor([user_idx] * len(candidate_movie_idxs))\n",
    "    movie_tensor = torch.tensor(candidate_movie_idxs)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = model(user_tensor, movie_tensor).cpu().numpy()\n",
    "    \n",
    "    # Top-k recommendations\n",
    "    #top_k_indices = np.argsort(scores)[-k:][::-1]\n",
    "    top_k_indices = np.argsort(scores)[-k:][::-1].copy()\n",
    "    top_k_movie_idxs = movie_tensor[top_k_indices].numpy()\n",
    "    \n",
    "    # Map back to movie_id\n",
    "    idx_to_movie = {idx: movie for movie, idx in movie_to_idx.items()}\n",
    "    top_k_movie_ids = [idx_to_movie[idx] for idx in top_k_movie_idxs]\n",
    "    \n",
    "    # Get movie titles\n",
    "    recommended_movies = movies_df[movies_df['movie_id'].isin(top_k_movie_ids)][['movie_id', 'title']]\n",
    "    recommended_movies['rank'] = recommended_movies['movie_id'].apply(lambda x: top_k_movie_ids.index(x))\n",
    "    \n",
    "    return recommended_movies.sort_values('rank').drop('rank', axis=1).reset_index(drop=True)\n",
    "\n",
    "user_id = 1  # Example user\n",
    "recommendations = recommend_for_user(model, user_id, user_to_idx, movie_to_idx, movies, ratings, k=10)\n",
    "print(recommendations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
