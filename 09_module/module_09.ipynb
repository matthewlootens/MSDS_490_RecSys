{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f3b403f",
   "metadata": {},
   "source": [
    "# Modeule 9: HSTU Recommender Engines\n",
    "\n",
    "This is a basic HSTU recommender engine that was trained on the 1 million MovieLens dataset.  This code was initially generated by ChatGPT (OpenAI 2025), before I debugged it and tested its expected behavior.\n",
    "\n",
    "OpenAI. 2025. *Chat with ChatGPT about building an HSTU-based recommender engine using PyTorch and MovieLens 1M*. May 30, 2025. ChatGPT. [https://chat.openai.com/](https://chat.openai.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf09948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "ratings = pd.read_csv(\"./data/ml-1m/ratings.dat\", sep=\"::\", engine=\"python\",\n",
    "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
    "\n",
    "movies = pd.read_csv(\"./data/ml-1m/movies.dat\", sep=\"::\", engine=\"python\",\n",
    "                      names=[\"movieId\", \"movieTitle\", \"genres\"],\n",
    "                      encoding=\"latin-1\")\n",
    "\n",
    "# Encode userId and movieId\n",
    "user_encoder = LabelEncoder()\n",
    "movie_encoder = LabelEncoder()\n",
    "\n",
    "ratings['userId'] = user_encoder.fit_transform(ratings['userId'])\n",
    "ratings['movieId'] = movie_encoder.fit_transform(ratings['movieId'])\n",
    "\n",
    "# Sort by timestamp for sequential modeling\n",
    "ratings.sort_values(by=['userId', 'timestamp'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb9adb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sessions(data, session_length=10):\n",
    "    sessions = []\n",
    "    targets = []\n",
    "    for uid, user_data in data.groupby('userId'):\n",
    "        movies = user_data['movieId'].tolist()\n",
    "        for i in range(0, len(movies) - session_length):\n",
    "            sessions.append(movies[i:i + session_length])\n",
    "            targets.append(movies[i + session_length])  # next movie\n",
    "    return sessions, targets\n",
    "\n",
    "sessions, targets = build_sessions(ratings, session_length=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6cf643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "#dataset = MovieLensDataset(sequences, targets)\n",
    "#dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2de8693",
   "metadata": {},
   "source": [
    "### Define the HSTU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17c12ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class HSTURecommender(nn.Module):\n",
    "    def __init__(self, num_movies, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_movies, embedding_dim)\n",
    "        \n",
    "        # Session-level RNN\n",
    "        self.session_rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # User-level summarization\n",
    "        self.user_linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.output = nn.Linear(hidden_dim, num_movies)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)  # [batch, seq_len, emb_dim]\n",
    "        _, h_n = self.session_rnn(emb)  # h_n: [1, batch, hidden_dim]\n",
    "        h_user = self.user_linear(h_n.squeeze(0))  # [batch, hidden_dim]\n",
    "        out = self.output(h_user)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae82857",
   "metadata": {},
   "source": [
    "### Create a test/train datasets and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "874d7fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 5.3021\n",
      "Epoch 20, Loss: 5.0716\n",
      "Epoch 30, Loss: 4.9445\n",
      "Epoch 40, Loss: 4.8635\n",
      "Epoch 50, Loss: 4.8008\n",
      "Epoch 60, Loss: 4.7552\n",
      "Epoch 70, Loss: 4.7141\n",
      "Epoch 80, Loss: 4.6778\n",
      "Epoch 90, Loss: 4.6543\n",
      "Epoch 100, Loss: 4.6307\n",
      "Epoch 110, Loss: 4.6147\n",
      "Epoch 120, Loss: 4.6103\n",
      "Epoch 130, Loss: 4.5806\n",
      "Epoch 140, Loss: 4.5850\n",
      "Epoch 150, Loss: 4.5553\n",
      "Epoch 160, Loss: 4.5463\n",
      "Epoch 170, Loss: 4.5403\n",
      "Epoch 180, Loss: 4.5299\n",
      "Epoch 190, Loss: 4.5257\n",
      "Epoch 200, Loss: 4.5204\n",
      "Epoch 210, Loss: 4.5218\n",
      "Epoch 220, Loss: 4.5296\n",
      "Epoch 230, Loss: 4.5083\n",
      "Epoch 240, Loss: 4.5033\n",
      "Epoch 250, Loss: 4.4999\n",
      "Epoch 260, Loss: 4.5160\n",
      "Epoch 270, Loss: 4.5078\n",
      "Epoch 280, Loss: 4.4906\n",
      "Epoch 290, Loss: 4.4898\n",
      "Epoch 300, Loss: 4.5057\n",
      "Epoch 310, Loss: 4.4911\n",
      "Epoch 320, Loss: 4.4840\n",
      "Epoch 330, Loss: 4.4894\n",
      "Epoch 340, Loss: 4.4914\n",
      "Epoch 350, Loss: 4.4813\n",
      "Epoch 360, Loss: 4.4991\n",
      "Epoch 370, Loss: 4.4928\n",
      "Epoch 380, Loss: 4.5191\n",
      "Epoch 390, Loss: 4.4887\n",
      "Epoch 400, Loss: 4.4744\n",
      "Epoch 410, Loss: 4.4725\n",
      "Epoch 420, Loss: 4.4809\n",
      "Epoch 430, Loss: 4.4731\n",
      "Epoch 440, Loss: 4.5064\n",
      "Epoch 450, Loss: 4.4772\n",
      "Epoch 460, Loss: 4.4756\n",
      "Epoch 470, Loss: 4.4723\n",
      "Epoch 480, Loss: 4.4692\n",
      "Epoch 490, Loss: 4.4672\n",
      "Epoch 500, Loss: 4.4629\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and test (90/10)\n",
    "train_seqs, test_seqs, train_targets, test_targets = train_test_split(\n",
    "    sessions, targets, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = MovieLensDataset(train_seqs, train_targets)\n",
    "test_dataset = MovieLensDataset(test_seqs, test_targets)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, num_workers=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, num_workers=8, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_hstu = HSTURecommender(num_movies=len(movie_encoder.classes_)).to(device)\n",
    "optimizer = torch.optim.Adam(model_hstu.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "    model_hstu.train()\n",
    "    total_loss = 0\n",
    "    #print(len(batch))\n",
    "    for batch in train_loader:\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model_hstu(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f15f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dcg_at_k(r, k):\n",
    "    r = np.asarray(r, dtype=np.float64)[:k]\n",
    "    if r.size:\n",
    "        return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "    return 0.\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    dcg = dcg_at_k(r, k)\n",
    "    ideal = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    return dcg / ideal if ideal > 0 else 0.\n",
    "\n",
    "def precision_at_k(r, k):\n",
    "    r = np.asarray(r, dtype=np.float64)[:k]\n",
    "    return np.mean(r)\n",
    "\n",
    "def recall_at_k(r, k, all_pos_num):\n",
    "    r = np.asarray(r, dtype=np.float64)[:k]\n",
    "    return np.sum(r) / all_pos_num if all_pos_num > 0 else 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b552f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device, top_ks=[10, 100]):\n",
    "    model.eval()\n",
    "    ndcg_scores = {k: [] for k in top_ks}\n",
    "    precision_scores = {k: [] for k in top_ks}\n",
    "    recall_scores = {k: [] for k in top_ks}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y_true in dataloader:\n",
    "            x = x.to(device)\n",
    "            y_true = y_true.to(device)\n",
    "\n",
    "            logits = model(x)  # shape: [batch, num_items]\n",
    "            topk_preds = torch.topk(logits, max(top_ks), dim=1).indices.cpu().numpy()\n",
    "\n",
    "            for i in range(len(x)):\n",
    "                true_item = y_true[i].item()\n",
    "                for k in top_ks:\n",
    "                    preds_k = topk_preds[i][:k]\n",
    "                    r = [1 if true_item == p else 0 for p in preds_k]\n",
    "                    ndcg_scores[k].append(ndcg_at_k(r, k))\n",
    "                    precision_scores[k].append(precision_at_k(r, k))\n",
    "                    recall_scores[k].append(recall_at_k(r, k, 1))  # only 1 positive item\n",
    "\n",
    "    print(\"Evaluation Results:\")\n",
    "    for k in top_ks:\n",
    "        print(f\"@{k}: NDCG={np.mean(ndcg_scores[k]):.4f}, \"\n",
    "              f\"Precision={np.mean(precision_scores[k]):.4f}, \"\n",
    "              f\"Recall={np.mean(recall_scores[k]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52526618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_user(model, user_sequence, movie_encoder, top_k=10):\n",
    "    model.eval()\n",
    "    input_seq = torch.tensor(user_sequence[-10:], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_seq)\n",
    "        topk = torch.topk(logits, top_k).indices.squeeze(0).cpu().tolist()\n",
    "    movie_ids = movie_encoder.inverse_transform(topk)\n",
    "    return movie_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccebff9",
   "metadata": {},
   "source": [
    "### Evaluate the HSTU model using the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59d4aac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "@10: NDCG=0.1137, Precision=0.0211, Recall=0.2112\n",
      "@100: NDCG=0.1920, Precision=0.0060, Recall=0.6045\n",
      "Recommended Movie IDs:       movieId                         movieTitle\n",
      "462       466        Hot Shots! Part Deux (1993)\n",
      "680       688        Operation Dumbo Drop (1995)\n",
      "1545     1586                   G.I. Jane (1997)\n",
      "2184     2253                        Toys (1992)\n",
      "2333     2402  Rambo: First Blood Part II (1985)\n",
      "2335     2404                   Rambo III (1988)\n",
      "2342     2411                    Rocky IV (1985)\n",
      "2746     2815                  Iron Eagle (1986)\n",
      "3372     3441                    Red Dawn (1984)\n",
      "3697     3766           Missing in Action (1984)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "evaluate(model_hstu, test_loader, device)\n",
    "\n",
    "# Recommend for a specific user sequence\n",
    "sample_seq = train_seqs[0]  # last 10 movieIds\n",
    "recommendations = recommend_for_user(model_hstu, sample_seq, movie_encoder)\n",
    "print(\"Recommended Movie IDs:\", movies[movies.movieId.isin(recommendations)].iloc[:, :-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a3640b",
   "metadata": {},
   "source": [
    "## DLMR RecSys\n",
    "\n",
    "This is a deep-learning recommender model that uses neural collaborative filtering. It was also trained on the 1 million MovieLens dataset.  The code for this was initially generated by ChatGPT (OpenAI 2025), before I then debugged it and confirmed it works as expected.\n",
    "\n",
    "OpenAI. 2025. Chat with ChatGPT about implementing a DLRM-based recommender system using PyTorch and MovieLens 1M. May 30, 2025. ChatGPT. https://chat.openai.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f11f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load MovieLens 1M dataset\n",
    "df = pd.read_csv('./data/ml-1m/ratings.dat', sep='::', engine='python',\n",
    "                 names=['userId', 'movieId', 'rating', 'timestamp'])\n",
    "\n",
    "# Binarize ratings (implicit feedback setup)\n",
    "df['rating'] = (df['rating'] >= 4).astype(int)\n",
    "\n",
    "# Encode user and item IDs\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "df['user'] = user_encoder.fit_transform(df['userId'])\n",
    "df['item'] = item_encoder.fit_transform(df['movieId'])\n",
    "\n",
    "num_users = df['user'].nunique()\n",
    "num_items = df['item'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58060ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "class MovieLensPairs(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = df['user'].values\n",
    "        self.items = df['item'].values\n",
    "        self.labels = df['rating'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.labels[idx]\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_ds = MovieLensPairs(train_df)\n",
    "test_ds = MovieLensPairs(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=1024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a7c527",
   "metadata": {},
   "source": [
    "### Define and train the NCF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bd281a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: loss = 0.5092\n",
      "Epoch 20: loss = 0.4125\n",
      "Epoch 30: loss = 0.3037\n",
      "Epoch 40: loss = 0.3335\n",
      "Epoch 50: loss = 0.3161\n",
      "Epoch 60: loss = 0.2891\n",
      "Epoch 70: loss = 0.1977\n",
      "Epoch 80: loss = 0.1686\n",
      "Epoch 90: loss = 0.1313\n",
      "Epoch 100: loss = 0.1932\n",
      "Epoch 110: loss = 0.2030\n",
      "Epoch 120: loss = 0.1081\n",
      "Epoch 130: loss = 0.1051\n",
      "Epoch 140: loss = 0.0841\n",
      "Epoch 150: loss = 0.1606\n",
      "Epoch 160: loss = 0.1219\n",
      "Epoch 170: loss = 0.1334\n",
      "Epoch 180: loss = 0.1916\n",
      "Epoch 190: loss = 0.0693\n",
      "Epoch 200: loss = 0.0370\n",
      "Epoch 210: loss = 0.0929\n",
      "Epoch 220: loss = 0.0979\n",
      "Epoch 230: loss = 0.0787\n",
      "Epoch 240: loss = 0.0881\n",
      "Epoch 250: loss = 0.0616\n"
     ]
    }
   ],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        user_emb = self.user_embedding(users)   # [B, D]\n",
    "        item_emb = self.item_embedding(items)   # [B, D]\n",
    "        x = torch.cat([user_emb, item_emb], dim=1)  # [B, 2D]\n",
    "        return self.mlp(x).squeeze(1)  # [B]\n",
    "\n",
    "\n",
    "\n",
    "model_ncf = NCF(num_users, num_items).to(device)\n",
    "optimizer = optim.Adam(model_ncf.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(250):\n",
    "    model_ncf.train()\n",
    "    for users, items, labels in train_loader:\n",
    "        users = users.to(device)\n",
    "        items = items.to(device)\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        logits = model_ncf(users, items)  # shape: [batch_size]\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}: loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63fe8309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics:\n",
      "Top-10 → NDCG: 0.0008, Precision: 0.0008, Recall: 0.0007\n",
      "Top-100 → NDCG: 0.0076, Precision: 0.0019, Recall: 0.0216\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def dcg_at_k(r, k):\n",
    "    r = np.asarray(r, dtype=np.float64)[:k]\n",
    "    if r.size:\n",
    "        return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "    return 0.\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    dcg = dcg_at_k(r, k)\n",
    "    ideal = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    return dcg / ideal if ideal > 0 else 0.\n",
    "\n",
    "def precision_at_k(r, k):\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.mean(r)\n",
    "\n",
    "def recall_at_k(r, k, all_positives):\n",
    "    r = np.asarray(r)[:k]\n",
    "    return np.sum(r) / all_positives if all_positives > 0 else 0.\n",
    "\n",
    "\n",
    "def evaluate_full_ranking(model, user_item_dict, all_items, device, top_ks=[10, 100]):\n",
    "    model.eval()\n",
    "\n",
    "    ndcg_all = {k: [] for k in top_ks}\n",
    "    prec_all = {k: [] for k in top_ks}\n",
    "    recall_all = {k: [] for k in top_ks}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for user, true_items in user_item_dict.items():\n",
    "            user_tensor = torch.LongTensor([user] * len(all_items)).to(device)\n",
    "            item_tensor = torch.LongTensor(all_items).to(device)\n",
    "\n",
    "            scores = model(user_tensor, item_tensor)  # [num_items]\n",
    "            scores = scores.cpu().numpy()\n",
    "\n",
    "            ranked_items = np.argsort(-scores)  # indices of all_items\n",
    "\n",
    "            hits = np.isin(all_items, list(true_items)).astype(int)\n",
    "            ranked_hits = hits[ranked_items]  # reordered according to model scores\n",
    "\n",
    "            for k in top_ks:\n",
    "                ndcg_all[k].append(ndcg_at_k(ranked_hits, k))\n",
    "                prec_all[k].append(precision_at_k(ranked_hits, k))\n",
    "                recall_all[k].append(recall_at_k(ranked_hits, k, all_positives=len(true_items)))\n",
    "\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    for k in top_ks:\n",
    "        print(f\"Top-{k} → NDCG: {np.mean(ndcg_all[k]):.4f}, Precision: {np.mean(prec_all[k]):.4f}, Recall: {np.mean(recall_all[k]):.4f}\")\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_user_item_dict_from_df(df):\n",
    "    user_item_dict = defaultdict(set)\n",
    "    for user, item, label in zip(df['user'].values, df['item'].values, df['rating'].values):\n",
    "        if label >= 1.0:  # assuming ratings ≥ 3 are positive interactions\n",
    "            user_item_dict[int(user)].add(int(item))\n",
    "    return dict(user_item_dict)\n",
    "\n",
    "user_item_dict = build_user_item_dict_from_df(test_df)\n",
    "all_items = sorted(list(set(train_df['item']).union(set(test_df['item']))))\n",
    "evaluate_full_ranking(model_ncf, user_item_dict, all_items, device=device, top_ks=[10, 100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "652b480f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Movie IDs:       movieId                                       movieTitle\n",
      "40         41                               Richard III (1995)\n",
      "52         53                                  Lamerica (1994)\n",
      "884       896                                Wild Reeds (1994)\n",
      "1782     1851                 Leather Jacket Love Story (1997)\n",
      "2434     2503                          Apple, The (Sib) (1998)\n",
      "2871     2940                                     Gilda (1946)\n",
      "2912     2981            Brother, Can You Spare a Dime? (1975)\n",
      "3020     3089  Bicycle Thief, The (Ladri di biciclette) (1948)\n",
      "3610     3679      Decline of Western Civilization, The (1981)\n",
      "3810     3880              Ballad of Ramblin' Jack, The (2000)\n"
     ]
    }
   ],
   "source": [
    "def recommend_ncf(model, user_id, user_encoder, item_encoder, top_k=10, device=device):\n",
    "    model.eval()\n",
    "\n",
    "    # Encode the user and create item indices\n",
    "    user_idx = torch.tensor([user_encoder.transform([user_id])[0]], dtype=torch.long).to(device)\n",
    "    all_item_indices = torch.arange(len(item_encoder.classes_), dtype=torch.long).to(device)\n",
    "\n",
    "    # Expand user to match number of items\n",
    "    user_tensor = user_idx.expand(all_item_indices.shape[0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = model(user_tensor, all_item_indices)  # shape: [num_items]\n",
    "        top_items = torch.topk(scores, top_k).indices.cpu().tolist()\n",
    "\n",
    "    # Decode recommended item indices to original item IDs\n",
    "    return item_encoder.inverse_transform(top_items)\n",
    "\n",
    "recommendations = recommend_ncf(\n",
    "    model_ncf,\n",
    "    user_id=5,\n",
    "    user_encoder=user_encoder,\n",
    "    item_encoder=item_encoder,  \n",
    "    top_k=10,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Recommended Movie IDs:\",\n",
    "      movies[movies.movieId.isin(recommendations)].iloc[:, :-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
